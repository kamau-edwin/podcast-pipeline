# Design Decisions in the Podcast Pipeline

This document captures the key design decisions I made during the development of the podcast pipeline. 

The aim was to balance **robustness**, **modularity**, **performance**, and **platform compatibility**.

---

## 1. Modular Python Classes

### Decision:
Split the pipeline into three core components:
- `DownloadPodcast`: Handles RSS parsing and audio downloads.
- `Transcriber`: Handles transcription and diarization using WhisperX.
- `PodcastDB`: Handles structured data persistence in PostgreSQL.

### Rationale:
- Promotes code reusability.
- Easier to test, update, and maintain each stage independently.
- Code extensibility (e.g. adding topic modeling to the transcription process in the future).

---
## 2. Submission of individual RSS URL 

### Decision:
Submit and process each RSS URL individually.

### Rationale:
- Enables parallel execution of the download and transcription process if multiple compute resources are available such as HPC
- Offers debuggability for an individual RSS URL.


## 3. Use of Local Intermediate Files (CSV, JSON)

### Decision:
Save metadata and transcript data to disk before database upload.

### Rationale:
- Enables resumability: If transcription or DB ingestion fails, files are already available.
- Avoids repeated processing 
- Supports parallel execution: JSON/CSV files can be processed independently.
- Offers transparency and debuggability for intermediate results.

---


## 4. Command-Line Interface (CLI) Entry points

### Decision:
Use `click` to expose `main.py` and `run_database.py` as CLI tools.

### Rationale:
- Easy to script, schedule, and use in local machine, HPC/slurm or cron
- Enables modular execution â€” e.g., run downloads separately from database uploads.


---

## 5. WhisperX for Transcription & Diarization

### Decision:
Use [WhisperX](https://github.com/m-bain/whisperx) for ASR and speaker diarization.

### Rationale:
- Combines transcription accuracy with speaker diarization.
- Supports GPU and CPU execution.
- Better alignment and speaker labeling than vanilla Whisper or other off-the-shelf options.

### Tradeoffs:
- Requires Hugging Face token for diarization models.
- Slightly higher system requirements (PyTorch, `ffmpeg`, etc.).

---

## 6. PostgreSQL as the Database Backend

### Decision:
Store metadata and transcripts in a PostgreSQL database.

### Rationale:
- Reliable, scalable, and easy to learn.
- Easy to query and integrate with other analytics tools.
- Easy to provide or revoke data access
- Supports complex queries and indexing.

### Alternatives Considered:
- No other alternatives were considered 

---

## 7. Pandas for I/O and Transformation

### Decision:
Use `pandas` to read/write CSV/JSON and interface with SQL database.

### Rationale:
- Easy for data manipulation.
- Integrates natively with SQLAlchemy and JSON/CSV formats.
- Supports batching, filtering, and transformation in-memory.

---

## 8. User-Supplied Hugging Face Token

### Decision:
Require users to generate their own Hugging Face token for model access.

### Rationale:
- WhisperX diarization depends on models that require user agreement.
- Free and provides useful error of why diarization fails.
- Reduces the risk of exposing sensitive tokens in source code.
- Ease of regenerating new token if token is compromised

---

## 9. Use of SQLAlchemy + psycopg2

### Decision:
Leverage SQLAlchemy for database connection and use `to_sql` for uploading data.

### Rationale:
- Abstraction over raw SQL queries.
- Integrates directly with `pandas`.
- Easier to manage DB connections and pool settings.

---


## 10. Manual Batch Execution

### Decision:
Run pipeline components sequentially (via CLI) rather than as one continuous script.

### Rationale:
- Ease of debugging, monitoring, and recovery if a step fails.
- Reduces Database upload conflict that would occur.

---



## Potential Pitfalls

| Issue                            | Mitigation Strategy                                   |
|----------------------------------|-------------------------------------------------------|
| Large file downloads/transcripts | Introduce batching, resume logic, or streaming        |
| Lack of retry/failure handling   | Add logging, try/except blocks, and retries per task  |
| Transcription bottlenecks        | Optimize batch_size and use GPU                       |

---

